\documentclass[12pt]{article}
\usepackage[canadien]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{fancyhdr}
\usepackage{graphicx}
\graphicspath{ {./logs/figures/} }
\usepackage{enumerate}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{subfigure}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[normalem]{ulem}
\useunder{\uline}{\ul}{}
\usepackage{float}
\usepackage{bm}
\usepackage{url}
\usepackage[dvipsnames]{xcolor}
\usepackage{todonotes}
\usepackage{hyperref}
\usepackage{monpackage}

% If your paper is accepted, change the options for the package
% aistats2e as follows:
%
%\usepackage[accepted]{aistats2e}
%
% This option will print headings for the title of your paper and
% headings for the authors names, plus a copyright note at the end of
% the first column of the first page.
\setlength{\parindent}{0cm}
\addtolength{\oddsidemargin}{-2cm}
\addtolength{\evensidemargin}{-2cm}
\setlength{\textwidth}{17.78cm}
\addtolength{\topmargin}{-2.25cm}
\setlength{\textheight}{23.24cm}
\addtolength{\parskip}{5mm}
\pagestyle{fancy}

%************
%* COMMANDS *
%************

\input{math_commands}

\newif\ifexercise
\exercisetrue
%\exercisefalse

\newif\ifsolution
\solutiontrue
%\solutionfalse

\usepackage{booktabs}
\usepackage[ruled,vlined]{algorithm2e}

\usepackage{amsthm}
\theoremstyle{definition}
\newtheorem{exercise}{Question}%[chapter]
\newtheorem{answer}{Answer} % asterisk to remove ordering

\newcommand{\Exercise}[1]{
\ifexercise#1\fi
}

\definecolor{answer}{rgb}{0.00, 0.12, 0.60}
\newcommand{\Answer}[1]{
\ifsolution\color{answer}\begin{answer}#1\end{answer}\fi
}

\newif\ifexercise
\exercisetrue
%\exercisefalse

\newif\ifsolution
\solutiontrue
%\solutionfalse

\usepackage{enumitem}
\newcommand{\staritem}{
\addtocounter{enumi}{1}
\item[$\phantom{x}^{*}$\theenumi]}
\setlist[enumerate,1]{leftmargin=*, label=\arabic*.}

\newcommand{\customcommandlreg}{\ensuremath{L_{\textnormal{reg}}(f(\mathbf{x}^{(i)}, \mathbf{\theta}), \mathbf{y}^{(i)})}}
\newcommand{\customcommandloss}{\ensuremath{L}(f(\mathbf{x}^{(i)}, \mathbf{\theta}), \mathbf{y}^{(i)})}

\def\vdelta{{\bm{\delta}}}
\usetikzlibrary{positioning}

\begin{document}


\fancyhead{}
\fancyfoot{}

\fancyhead[L]{
  \begin{tabular}[b]{l}
    IFT6135-A2023  \\
    Prof: Aishwarya Agrawal \\
  \end{tabular}
}
\fancyhead[R]{
  \begin{tabular}[b]{r}
    Assignment 2, Theoretical Part   \\
    RNN, Optimization, Regularization, Transformers, Normalization\\
  \end{tabular}
}
\fancyfoot[C]{- Do not distribute -}

\vspace{1cm}

\shorthandoff{:}
{\textbf{Due Date: November 14th, 2023 at 11:00 pm}}\\


\vspace{-0.5cm}
\underline{Instructions}%
\renewcommand{\labelitemi}{\textbullet}

\begin{itemize}
\item \emph{For all questions, show your work!}
\item \emph{Use LaTeX and the template we provide when writing your answers.
You may reuse most of the notation shorthands, equations and/or tables.
See the assignment policy on the course website for more details.}
\item \emph{The use of AI tools like Chat-GPT to find answers or parts of answers for any question in this assignment is not allowed. However, you can use these tools to improve the quality of your writing, like fixing grammar or making it more understandable. If you do use these tools, you must clearly explain how you used them and which questions or parts of questions you applied them to. Failing to do so or using these tools to find answers or parts of answers may result in your work being completely rejected, which means you'll receive a score of 0 for the entire theory or practical section.}
\item \emph{Submit your answers electronically via Gradescope.}
\end{itemize}
\subsection*{Problem 2}
\subsubsection*{Problem 2.5}
My modules are all linear and are composed of 4 different
layers:$(W_{k}, W_{v}, W_{q}, W_{o})$ with bias terms. Each of these square matrix is
of size $(numheads \times headsize)^{2}$ and has a bias. Therefore, there are
$$4 \pr{(numheads \times headsize)^{2} + numheads \times headsize}$$

\subsection*{Problem 3}
\subsubsection*{Problem 3.1}

These are the lowest validation perplexity score across epochs:
\begin{enumerate}
  \item gpt1 layer 1 adam minimal validation perplexity: 148.970
  \item gpt1 layer 1 adamw min validation perplexity: 148.512
  \item gpt1 layer 1 momentum min validation perplexity: 684.063
  \item gpt1 layer 1 sgd min validation perplexity: 1502.892
  \item gpt1 layer 2 adamw min validation perplexity: 152.072
  \item gpt1 layer 4 adamw min validation perplexity: 1486.629
  \item lstm layer 1 adam min validation perplexity: 144.289
  \item lstm layer 1 adamw min validation perplexity: 144.154
  \item lstm layer 1 momentum min validation perplexity: 1654.189
  \item lstm layer 1 sgd min validation perplexity: 2173.387
  \item lstm layer 2 adamw min validation perplexity: 139.819
  \item lstm layer 4 adamw min validation perplexity: 160.328
\end{enumerate}

We also have obtained the following figures:

\begin{figure}[H]
     \centering
     \includegraphics[scale=0.4]{gpt1_layer_1_adam.png}
     \caption{gpt1 with 1 layer and optmized with adam perplexity over time and epochs}
\end{figure}
\begin{figure}[H]
     \centering
     \includegraphics[scale=0.4]{gpt1_layer_1_adamw.png}
     \caption{gpt1 with 1 layer and optmized with adamw perplexity over time and epochs}
\end{figure}
\begin{figure}[H]
     \centering
     \includegraphics[scale=0.4]{gpt1_layer_1_momentum.png}
     \caption{gpt1 with 1 layer and optmized with momentum perplexity over time and epochs}
\end{figure}
\begin{figure}[H]
     \centering
     \includegraphics[scale=0.4]{gpt1_layer_1_sgd.png}
     \caption{gpt1 with 1 layer and optmized with sgd perplexity over time and epochs}
\end{figure}
\begin{figure}[H]
     \centering
     \includegraphics[scale=0.4]{lstm_layer_1_adam.png}
     \caption{lstm with 1 layer and optmized with adam perplexity over time and epochs}
\end{figure}
\begin{figure}[H]
     \centering
     \includegraphics[scale=0.4]{lstm_layer_1_adamw.png}
     \caption{lstm with 1 layer and optmized with adamw perplexity over time and epochs}
\end{figure}
\begin{figure}[H]
     \centering
     \includegraphics[scale=0.4]{lstm_layer_1_momentum.png}
     \caption{lstm with 1 layer and optmized with momentum perplexity over time and epochs}
\end{figure}
\begin{figure}[H]
     \centering
     \includegraphics[scale=0.4]{lstm_layer_1_sgd.png}
     \caption{lstm with 1 layer and optmized with sgd perplexity over time and epochs}
\end{figure}
\begin{figure}[H]
     \centering
     \includegraphics[scale=0.4]{gpt1_layer_2_adamw.png}
     \caption{gpt1 with 2 layer and optmized with adamw perplexity over time and epochs}
\end{figure}
\begin{figure}[H]
     \centering
     \includegraphics[scale=0.4]{gpt1_layer_4_adamw.png}
     \caption{gpt1 with 4 layer and optmized with adamw perplexity over time and epochs}
\end{figure}
\begin{figure}[H]
     \centering
     \includegraphics[scale=0.4]{lstm_layer_2_adamw.png}
     \caption{lstm with 2 layer and optmized with adamw perplexity over time and epochs}
\end{figure}
\begin{figure}[H]
     \centering
     \includegraphics[scale=0.4]{lstm_layer_4_adamw.png}
     \caption{lstm with 4 layer and optmized with adamw perplexity over time and epochs}
\end{figure}

\subsubsection*{Problem 3.2}
Here are the results of the 12 experiments:
\begin{table}[H]
\resizebox{\textwidth}{!}{%
\begin{tabular}{llllllllll}
Exp & Architecture &  \#layers & Optimizer &Train Loss & Train PPL & Validation Loss & Validation PPL & Test Loss & Test PPL \\ \hline
1  & LSTM & 1 & Adam      &4.399 & 81.436 & 4.971 & 144.289 & 4.995 & 147.777 \\
2  & \bf{LSTM} & \bf{1} & \bf{Adamw}     &\bf{4.392} & \bf{80.854} & \bf{4.970} & \bf{144.154} & \bf{5.001} & \bf{148.673} \\
3  & LSTM & 1 & SGD       &7.750 & 2322.216 & 7.684 & 2173.387 &  7.644 & 2088.849 \\
4  & LSTM & 1 & Momentum  &7.446 & 1748.463 & 7.411 & 1654.189 & 7.377 & 1600.139 \\
9  & LSTM & 2 & Adamw     &4.0733 & 58.754 & 4.986 & 146.357 & 5.021 & 151.656 \\
10 & LSTM & 4 & Adamw     &4.470 & 87.406 & 5.086 & 161.846 & 5.106 & 165.131 \\
5  & GPT1 & 1 & Adam      &3.765 & 43.19 & 5.16 & 174.650 & 5.182 & 178.151 \\
6  &\bf{GPT1} & \bf{1}& \bf{Adamw}     &\bf{3.752}&\bf{42.634} &\bf{5.157} & \bf{173.79} & \bf{5.180} & \bf{177.837} \\
7  & GPT1 & 1 & SGD       &7.381 & 1605.992 & 7.315 & 1502.892 & 7.291 & 1467.53 \\
8  & GPT1 & 1 & Momentum  &6.583 & 715.571 & 6.528 & 684.063 & 6.504 & 667.813 \\
11 & GPT1 & 2 & Adamw     &4.167 & 64.525 & 5.026 & 152.330 & 5.023 & 151.896 \\
12 & GPT1 & 4 & Adamw     &7.347 & 1552.593 & 7.304 & 1486.629 & 7.274 & 1443.59
\end{tabular}}
\caption{Results for the 12 experiments. Each models were trained on 10 epochs}
\end{table}

\subsubsection*{Problem 3.3}
If the goal was to minimize the training time of a model, I would choose either
the SGD optimizer or the momentum optmizer with an LSTM. They were the fastest
to train. If the goal was to generalize well, I would chose the LSTM with Adam
or Adamw or GPT1 with Adamw or Adam. These four models had the best test loss for a
single layer model and were all close to each others with respect to the
test loss. Their test perplexity was generally the lowest too.
\subsubsection*{Problem 3.4}
Both SGD and momentum generate high validation perplexity and do not generalize
as well as Adam and Adamw, when comparing the test loss and the test perplexity.
The high precision of Adam and Adamw came at a slightly higher training time.
\end{document}
